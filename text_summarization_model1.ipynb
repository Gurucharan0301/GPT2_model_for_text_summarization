{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "data=pd.read_parquet('train-00000-of-00003.parquet')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the model has any padding token initialized, if not then add a padding token \n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer the input and the target text\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,data,tokenizer,max_len):\n",
    "    self.data=data\n",
    "    self.tokenizer=tokenizer\n",
    "    self.max_len=max_len\n",
    "  def __getitem__(self,index):\n",
    "    feature=self.data.article[index]\n",
    "    label=self.data.highlights[index]\n",
    "    encoding=self.tokenizer.encode_plus(feature,label)\n",
    "    length=len(encoding['input_ids'])\n",
    "    target=self.tokenizer.encode(label)\n",
    "    length_target=len(target)\n",
    "    diff_1=abs(length-self.max_len)\n",
    "    diff_2=abs(length_target-self.max_len)\n",
    "    input_id=encoding['input_ids']+[50256]*diff_1\n",
    "    mask=encoding['attention_mask']+[0]*diff_1\n",
    "    targets=target+[50256]*diff_2\n",
    "    return{\n",
    "        'input_ids':torch.tensor(input_id),'mask':torch.tensor(mask),'target':torch.tensor(targets)\n",
    "    }\n",
    "  def __len__(self):\n",
    "    return len(data)\n",
    "x=CustomDataset(data1,tokenizer,max_len=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing optimizer\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training part of the code \n",
    "batch_size=10\n",
    "data_len=len(x)\n",
    "max_len=3000\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    num_batches = 0    \n",
    "\n",
    "    for batch in range(0,data_len,batch_size):\n",
    "        end=min(batch+batch_size,data_len)\n",
    "        batch_samples=[x[i] for i in range(batch,end)]\n",
    "        batch_loss=0\n",
    "        print(len(batch_samples))\n",
    "        for sample in batch_samples:\n",
    "            sample=dict(sample)\n",
    "            b=[sample[j][k:k+1000] for k in range(0,max_len,1000) for j in sample]\n",
    "            for l in range(0,len(b),3):\n",
    "                input_ids=b[l]\n",
    "                mask=b[l+1]\n",
    "                target=b[l+2]\n",
    "                model.train()\n",
    "                output=model(input_ids=input_ids,attention_mask=mask,labels=target)    \n",
    "                loss=output.loss\n",
    "                batch_loss+=loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        total_loss += batch_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Batch {num_batches}, Loss: {batch_loss:.4f}\")\n",
    "\n",
    "    epoch_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(f\"Total samples processed: {data_len}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
